{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = 'Frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "BEAM_SIZE = 8\n",
    "NUM_LETTERS = 11\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# Make lower case and split into individual words.\n",
    "text = text.lower()\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace(' ', ' ')\n",
    "# Encode characters as indices.\n",
    "unique_chars = list(set(text))\n",
    "char_to_index = dict((ch, index) for index,\n",
    "                                ch in enumerate(unique_chars))\n",
    "index_to_char = dict((index, ch) for index,\n",
    "                                ch in enumerate(unique_chars))\n",
    "encoding_width = len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n",
    "# Convert to one-hot encoded training data.\n",
    "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width))\n",
    "y = np.zeros((len(fragments), encoding_width))\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for j, char in enumerate(fragment):\n",
    "        X[i, j, char_to_index[char]] = 1\n",
    "    target_char = targets[i]\n",
    "    y[i, char_to_index[target_char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 128)         99840     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 66)                8514      \n",
      "=================================================================\n",
      "Total params: 239,938\n",
      "Trainable params: 239,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 138944 samples, validate on 7313 samples\n",
      "Epoch 1/50\n",
      "138944/138944 - 51s - loss: 2.8049 - val_loss: 2.6683\n",
      "Epoch 2/50\n",
      "138944/138944 - 52s - loss: 2.3857 - val_loss: 2.5303\n",
      "Epoch 3/50\n",
      "138944/138944 - 53s - loss: 2.2400 - val_loss: 2.4263\n",
      "Epoch 4/50\n",
      "138944/138944 - 59s - loss: 2.1499 - val_loss: 2.3650\n",
      "Epoch 5/50\n",
      "138944/138944 - 61s - loss: 2.0812 - val_loss: 2.3307\n",
      "Epoch 6/50\n",
      "138944/138944 - 61s - loss: 2.0329 - val_loss: 2.3075\n",
      "Epoch 7/50\n",
      "138944/138944 - 64s - loss: 1.9894 - val_loss: 2.2657\n",
      "Epoch 8/50\n",
      "138944/138944 - 65s - loss: 1.9531 - val_loss: 2.2439\n",
      "Epoch 9/50\n",
      "138944/138944 - 61s - loss: 1.9211 - val_loss: 2.2258\n",
      "Epoch 10/50\n",
      "138944/138944 - 67s - loss: 1.8954 - val_loss: 2.2104\n",
      "Epoch 11/50\n",
      "138944/138944 - 63s - loss: 1.8707 - val_loss: 2.2010\n",
      "Epoch 12/50\n",
      "138944/138944 - 61s - loss: 1.8524 - val_loss: 2.1832\n",
      "Epoch 13/50\n",
      "138944/138944 - 63s - loss: 1.8274 - val_loss: 2.1749\n",
      "Epoch 14/50\n",
      "138944/138944 - 65s - loss: 1.8095 - val_loss: 2.1530\n",
      "Epoch 15/50\n",
      "138944/138944 - 61s - loss: 1.7929 - val_loss: 2.1523\n",
      "Epoch 16/50\n",
      "138944/138944 - 64s - loss: 1.7769 - val_loss: 2.1543\n",
      "Epoch 17/50\n",
      "138944/138944 - 87s - loss: 1.7633 - val_loss: 2.1536\n",
      "Epoch 18/50\n",
      "138944/138944 - 59s - loss: 1.7508 - val_loss: 2.1281\n",
      "Epoch 19/50\n",
      "138944/138944 - 62s - loss: 1.7362 - val_loss: 2.1249\n",
      "Epoch 20/50\n",
      "138944/138944 - 84s - loss: 1.7234 - val_loss: 2.1242\n",
      "Epoch 21/50\n",
      "138944/138944 - 61s - loss: 1.7138 - val_loss: 2.1279\n",
      "Epoch 22/50\n",
      "138944/138944 - 64s - loss: 2.0156 - val_loss: 2.5087\n",
      "Epoch 23/50\n",
      "138944/138944 - 88s - loss: 2.2217 - val_loss: 2.4145\n",
      "Epoch 24/50\n",
      "138944/138944 - 61s - loss: 2.0873 - val_loss: 2.3195\n",
      "Epoch 25/50\n",
      "138944/138944 - 65s - loss: 2.0356 - val_loss: 2.3601\n",
      "Epoch 26/50\n",
      "138944/138944 - 88s - loss: 2.1173 - val_loss: 2.3229\n",
      "Epoch 27/50\n",
      "138944/138944 - 59s - loss: 2.0614 - val_loss: 2.2995\n",
      "Epoch 28/50\n",
      "138944/138944 - 92s - loss: 2.0185 - val_loss: 2.2821\n",
      "Epoch 29/50\n",
      "138944/138944 - 62s - loss: 1.9818 - val_loss: 2.2602\n",
      "Epoch 30/50\n",
      "138944/138944 - 60s - loss: 1.9430 - val_loss: 2.2310\n",
      "Epoch 31/50\n",
      "138944/138944 - 86s - loss: 1.9114 - val_loss: 2.2109\n",
      "Epoch 32/50\n",
      "138944/138944 - 62s - loss: 1.9010 - val_loss: 2.2051\n",
      "Epoch 33/50\n",
      "138944/138944 - 89s - loss: 1.8786 - val_loss: 2.2010\n",
      "Epoch 34/50\n",
      "138944/138944 - 59s - loss: 1.8569 - val_loss: 2.1857\n",
      "Epoch 35/50\n",
      "138944/138944 - 93s - loss: 1.8402 - val_loss: 2.1724\n",
      "Epoch 36/50\n",
      "138944/138944 - 55s - loss: 1.8234 - val_loss: 2.1695\n",
      "Epoch 37/50\n",
      "138944/138944 - 62s - loss: 1.8095 - val_loss: 2.1627\n",
      "Epoch 38/50\n",
      "138944/138944 - 83s - loss: 1.7929 - val_loss: 2.1540\n",
      "Epoch 39/50\n",
      "138944/138944 - 64s - loss: 1.7767 - val_loss: 2.1333\n",
      "Epoch 40/50\n",
      "138944/138944 - 87s - loss: 1.7585 - val_loss: 2.1415\n",
      "Epoch 41/50\n",
      "138944/138944 - 60s - loss: 1.7461 - val_loss: 2.1172\n",
      "Epoch 42/50\n",
      "138944/138944 - 90s - loss: 1.7327 - val_loss: 2.1178\n",
      "Epoch 43/50\n",
      "138944/138944 - 59s - loss: 1.7195 - val_loss: 2.1205\n",
      "Epoch 44/50\n",
      "138944/138944 - 89s - loss: 1.7064 - val_loss: 2.1023\n",
      "Epoch 45/50\n",
      "138944/138944 - 59s - loss: 1.6962 - val_loss: 2.0904\n",
      "Epoch 46/50\n",
      "138944/138944 - 61s - loss: 1.6836 - val_loss: 2.0952\n",
      "Epoch 47/50\n",
      "138944/138944 - 85s - loss: 1.6756 - val_loss: 2.0991\n",
      "Epoch 48/50\n",
      "138944/138944 - 59s - loss: 1.6663 - val_loss: 2.0834\n",
      "Epoch 49/50\n",
      "138944/138944 - 91s - loss: 1.6533 - val_loss: 2.0901\n",
      "Epoch 50/50\n",
      "138944/138944 - 58s - loss: 1.6428 - val_loss: 2.0741\n"
     ]
    }
   ],
   "source": [
    "# Build and train model.\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True,\n",
    "dropout=0.2, recurrent_dropout=0.2,\n",
    "input_shape=(None, encoding_width)))\n",
    "model.add(LSTM(128, dropout=0.2,\n",
    "recurrent_dropout=0.2))\n",
    "model.add(Dense(encoding_width, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 138944 samples, validate on 7313 samples\n",
      "Epoch 1/50\n",
      "138944/138944 - 48s - loss: 1.6342 - val_loss: 2.0750\n",
      "Epoch 2/50\n",
      "138944/138944 - 50s - loss: 1.6267 - val_loss: 2.0771\n",
      "Epoch 3/50\n",
      "138944/138944 - 57s - loss: 1.6208 - val_loss: 2.0706\n",
      "Epoch 4/50\n",
      "138944/138944 - 57s - loss: 1.6119 - val_loss: 2.0657\n",
      "Epoch 5/50\n",
      "138944/138944 - 66s - loss: 1.6029 - val_loss: 2.0701\n",
      "Epoch 6/50\n",
      "138944/138944 - 63s - loss: 1.5959 - val_loss: 2.0651\n",
      "Epoch 7/50\n",
      "138944/138944 - 62s - loss: 1.5891 - val_loss: 2.0578\n",
      "Epoch 8/50\n",
      "138944/138944 - 63s - loss: 1.5832 - val_loss: 2.0472\n",
      "Epoch 9/50\n",
      "138944/138944 - 67s - loss: 1.5753 - val_loss: 2.0576\n",
      "Epoch 10/50\n",
      "138944/138944 - 62s - loss: 1.5698 - val_loss: 2.0530\n",
      "Epoch 11/50\n",
      "138944/138944 - 65s - loss: 1.5645 - val_loss: 2.0488\n",
      "Epoch 12/50\n",
      "138944/138944 - 68s - loss: 1.5564 - val_loss: 2.0408\n",
      "Epoch 13/50\n",
      "138944/138944 - 62s - loss: 1.5530 - val_loss: 2.0480\n",
      "Epoch 14/50\n",
      "138944/138944 - 62s - loss: 1.5469 - val_loss: 2.0489\n",
      "Epoch 15/50\n",
      "138944/138944 - 67s - loss: 1.5436 - val_loss: 2.0564\n",
      "Epoch 16/50\n",
      "138944/138944 - 62s - loss: 1.5354 - val_loss: 2.0491\n",
      "Epoch 17/50\n",
      "138944/138944 - 63s - loss: 1.5327 - val_loss: 2.0474\n",
      "Epoch 18/50\n",
      "138944/138944 - 66s - loss: 1.5255 - val_loss: 2.0465\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5396\\2337401993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     shuffle=True)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, validation_split=0.05,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=2,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice siddodagy: \n",
      "alice siddodagy:”\n",
      "alice siddodagy:,\n",
      "alice siddodagy:_\n",
      "alice siddodagy:.\n",
      "alice siddodagy:d\n",
      "alice siddodagy:s\n",
      "alice siddodagy:’\n"
     ]
    }
   ],
   "source": [
    "letters = 'alice '\n",
    "one_hots = []\n",
    "for i, char in enumerate(letters):\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "beams = [(np.log(1.0), letters, one_hots)]\n",
    "# Predict NUM_LETTERS into the future.\n",
    "for i in range(NUM_LETTERS):\n",
    "    minibatch_list = []\n",
    "    # Create minibatch from one-hot encodings, and predict.\n",
    "    for triple in beams:\n",
    "        minibatch_list.append(triple[2])\n",
    "    minibatch = np.array(minibatch_list)\n",
    "    y_predict = model.predict(minibatch, verbose=0)\n",
    "    new_beams = []\n",
    "    for j, softmax_vec in enumerate(y_predict):\n",
    "        triple = beams[j]\n",
    "    # Create BEAM_SIZE new beams from each existing beam.\n",
    "    for k in range(BEAM_SIZE):\n",
    "        char_index = np.argmax(softmax_vec)\n",
    "        new_prob = triple[0] + np.log(\n",
    "        softmax_vec[char_index])\n",
    "        new_letters = triple[1] + index_to_char[char_index]\n",
    "        x = np.zeros(encoding_width)\n",
    "        x[char_index] = 1\n",
    "        new_one_hots = triple[2].copy()\n",
    "        new_one_hots.append(x)\n",
    "        new_beams.append((new_prob, new_letters,\n",
    "        new_one_hots))\n",
    "        softmax_vec[char_index] = 0\n",
    "# Prune tree to only keep BEAM_SIZE most probable beams.\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    beams = new_beams[0:BEAM_SIZE]\n",
    "for item in beams:\n",
    "    print(item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "direct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
